# **Описание эксперимент
В работе исследуется производительность параллельной реализации вычисления функционала вида

y=max minaij
1≤i≤n 1≤j≤n

Рассматривались четыре варианта структуры матрицы:
dense — полностью заполненная матрица;
upper — верхнетреугольная;
lower — нижнетреугольная;
band — ленточная матрица с фиксированной полушириной ленты.

Для каждого типа матриц фиксировались:

    время выполнения алгоритма;
    ускорение относительно последовательной версии;
    зависимость этих величин от числа потоков (множество значений включало конфигурации от 1 до числа доступных ядер и выше).​

#Постановка и реализация
В исходной задаче по каждой строке матрицы ищется минимум элементов, после чего среди всех строк выбирается максимальное из найденных минимумов.

Последовательный вариант использует два вложенных цикла: внешний бежит по строкам, внутренний — по допустимому диапазону столбцов с учётом формы матрицы; для каждой строки накапливается локальный минимум, затем обновляется глобальный максимум по строкам.​

Параллельный вариант распределяет строки между потоками с помощью директивы OpenMP parallel for и редукции по операции max, чтобы корректно объединить локальные максимумы, полученные в разных нитях. Используются различные схемы расписания (static, dynamic, guided), что позволяет сравнить влияние стратегии раздачи итераций на итоговое время и ускорение.​

**Анализ поведения по графикам
Ниже даётся интерпретация усреднённых графиков времени и ускорения для каждого класса матриц; конкретные изображения берутся из файлов, сгенерированных скриптом plot.py (папки plots/dense, plots/lower, plots/band).​

Полносвязная матрица (dense)
На графиках для dense‑матриц время вычислений заметно уменьшается при переходе от одного потока к нескольким, однако после 10–12 потоков снижение практически прекращается, а кривая ускорения выходит на плато около 7–8 раз. Это связано с тем, что все строки имеют одинаковую длину, нагрузка между нитями распределяется ровно, и дальнейший рост числа потоков ограничивается пропускной способностью памяти и накладными расходами синхронизации.​

Верхняя и нижняя треугольные (upper, lower)
Для треугольных матриц длина строки существенно меняется от начала к концу: часть нитей получает более тяжёлые строки, часть — короткие. На графиках видно, что при статическом расписании время заметно выше, а ускорение ниже, чем при dynamic или guided, особенно при большом числе потоков. Динамические схемы, напротив, перераспределяют работу по мере выполнения, что приводит к лучшему выравниванию нагрузки и ускорению порядка 7–8 раз на крупных размерах n.​

Ленточная матрица (band)
У ленточной матрицы число обрабатываемых элементов в строке ограничено шириной ленты, поэтому фактический объём вычислений на строку меньше, чем у dense‑варианта. На графиках band‑матриц видно, что время исполнения падает быстрее всего, а кривые ускорения для static и guided близки к линейному росту до 10–12 потоков. Здесь память не становится «бутылочным горлышком», и выигрыш от распараллеливания проявляется сильнее, хотя при очень большом числе потоков начинает сказываться накладной расход OpenMP.​

Сводные замечания
Для структурированных матриц (upper, lower, band) выбор стратегии расписания критичен: dynamic и guided практически всегда дают меньшее время и большее ускорение по сравнению со static, за счёт лучшего баланса нагрузки между потоками.​

Для тяжёлого dense‑случая масштабирование ограничено подсистемой памяти: после определённого числа потоков рост ускорения замедляется, хотя абсолютное время всё равно уменьшается.​

В среднем параллельная реализация даёт ускорение от 3–4 раз на небольших задачах до 7–8 раз на крупных размерах матриц, причём максимальный эффект достигается для ленточных и треугольных матриц при использовании динамических схем распределения итераций.​

добавь пример кода и сделай все формулы в виде латеха
Описание эксперимента
В работе изучается масштабирование параллельного вычисления функционала
y=max minaij
1≤i≤n 1≤j≤n
то есть максимума по строкам от их покомпонентных минимумов. Рассматриваются четыре варианта структуры матрицы: полная 
dense, верхнетреугольная upper, нижнетреугольная lower и ленточная 
band с заданной полушириной ленты.​

Для каждого типа матриц измеряются время выполнения 
Tpar(p) при числе потоков p и ускорение
S(p)=Tseq/Tpar(p),
где Tseq — среднее время последовательного варианта.​

Последовательный и параллельный алгоритмы
В последовательной реализации по каждой строке i вычисляется минимум по допустимому диапазону столбцов 
[j0(i),j1(i)], определяемому типом матрицы, после чего глобальный максимум по строкам обновляется значением 
mi=minaij.​
j0(i)≤j≤j1(i)

double seq_maximin(const std::vector<double>& a,
                   int rows, int cols,
                   Shape sh, int bw)
{
    double best = -std::numeric_limits<double>::infinity();

    for (int i = 0; i < rows; ++i) {
        int j0, j1;
        cols_for_row(i, cols, sh, bw, j0, j1); // выбираем диапазон столбцов
        if (j0 > j1) continue;

        double row_min = std::numeric_limits<double>::infinity();
        int base = i * cols;
        for (int j = j0; j <= j1; ++j) {
            double v = a[base + j];
            if (v < row_min) row_min = v;
        }
        if (row_min > best) best = row_min;
    }

    return best;
}
Параллельная версия распределяет строки между потоками с помощью директивы OpenMP и использует редукцию по операции 
max, чтобы корректно объединить локальные значения mi, вычисленные в разных нитях.​

double omp_maximin(const std::vector<double>& a,
                   int rows, int cols,
                   Shape sh, int bw,
                   int threads,
                   SchedPolicy pol,
                   int chunk)
{
    double best = -std::numeric_limits<double>::infinity();
    omp_set_num_threads(threads);

    omp_sched_t kind = (pol == SchedPolicy::Static)  ? omp_sched_static  :
                       (pol == SchedPolicy::Dynamic) ? omp_sched_dynamic :
                                                     omp_sched_guided;
    omp_set_schedule(kind, chunk > 0 ? chunk : 0);

    #pragma omp parallel for schedule(runtime) reduction(max : best)
    for (int i = 0; i < rows; ++i) {
        int j0, j1;
        cols_for_row(i, cols, sh, bw, j0, j1);
        if (j0 > j1) continue;

        double row_min = std::numeric_limits<double>::infinity();
        int base = i * cols;
        for (int j = j0; j <= j1; ++j) {
            double v = a[base + j];
            if (v < row_min) row_min = v;
        }
        if (row_min > best) best = row_min;
    }

    return best;
}
Интерпретация графиков
Для dense‑матриц зависимости 
Tpar(p) и S(p) показывают быстрый выигрыш при переходе от p=1 к нескольким потокам и последующее плато ускорения на уровне примерно S(p)≈7–8, что указывает на ограничение пропускной способностью памяти при равномерной нагрузке по строкам.​
Для треугольных матриц upper и lower длина строк различается, поэтому статическое расписание приводит к перекосу нагрузки, тогда как схемы dynamic и guided дают меньшие времена и ускорения до S(p)≈7–8 на крупных размерах n за счёт перераспределения итераций между нитями.​

В случае ленточной матрицы band эффективное число элементов в строке невелико, поэтому время вычислений уменьшается наиболее резко, а графики S(p) для static и guided близки к линейному росту до 10–12 потоков; здесь подсистема памяти не становится узким местом, и основной эффект начинает давать накладной расход параллелизма при слишком большом p.​

**Сводные выводы
Параллельная реализация вычисления
y=max( min aij )
  ⁡1≤i≤n  1≤j≤n

обеспечивает ускорение от S(p)≈3 на небольших задачах до S(p)≈7–8 на крупных dense, upper, lower и band‑матрицах, причём на нерегулярных структурах (upper, lower, band) выбор расписания 
dynamic или guided критически улучшает баланс нагрузки и итоговое время выполнения.