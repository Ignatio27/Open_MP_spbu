# Постановка эксперимента

Цель работы — оценить, как распараллеливание поиска минимального и максимального элементов большого массива вещественных чисел с помощью OpenMP влияет на время выполнения при разном количестве потоков и размерах задачи. Рассматриваются две параллельные реализации и однопоточный эталон.

Эксперименты проводились для массивов длиной \(10^5\), \(10^6\), \(5 \cdot 10^6\) и \(2 \cdot 10^7\) элементов. Для каждого размера выполнялись запуски при числе потоков от 1 до 20. Для каждого набора параметров измерялось среднее время выполнения и вычислялось ускорение относительно однопоточного варианта. По данным из `bm_table.csv` построены графики времени и ускорения (файлы в каталоге `plots/`).

# Реализация алгоритмов

## Последовательный эталон

В базовой реализации массив просматривается один раз в одном потоке: на каждом шаге текущий элемент сравнивается с сохранёнными значениями минимума и максимума, при необходимости они обновляются. Эта версия не содержит накладных расходов на создание потоков и синхронизацию и используется как эталон для расчёта ускорения.

## Параллельный вариант с редукцией

Первый параллельный вариант использует директиву `parallel for` с двумя редукциями: по минимуму и по максимуму. Каждый поток обрабатывает свою часть массива и ведёт локальные копии искомых экстремумов, а рантайм OpenMP после завершения цикла автоматически объединяет локальные значения в глобальный результат.

Пример кода:
struct Extrema {
double min_val;
double max_val;
};

Extrema run_with_reduction(const std::vector<double> &data, int workers) {
double gmin = std::numeric_limits<double>::infinity();
double gmax = -std::numeric_limits<double>::infinity();
omp_set_num_threads(workers);
#pragma omp parallel for reduction(min : gmin) reduction(max : gmax) schedule(static)
for (int i = 0; i < static_cast<int>(data.size()); ++i) {
double v = data[i];
if (v < gmin) gmin = v;
if (v > gmax) gmax = v;
}
return {gmin, gmax};


Такой подход не требует явных критических секций и уменьшает долю времени, затрачиваемого на синхронизацию.

## Параллельный вариант с локальными экстремумами

Во второй реализации создаётся параллельная область, внутри которой каждый поток вычисляет свои локальные значения минимума и максимума. После распределённого цикла результаты потоков собираются вручную в короткой критической секции.

Пример кода:

Extrema run_with_locals(const std::vector<double> &data, int workers) {
double gmin = std::numeric_limits<double>::infinity();
double gmax = -std::numeric_limits<double>::infinity();
omp_set_num_threads(workers);
#pragma omp parallel
{
double lmin = std::numeric_limits<double>::infinity();
double lmax = -std::numeric_limits<double>::infinity();

#pragma omp for schedule(static)
for (int i = 0; i < static_cast<int>(data.size()); ++i) {
double v = data[i];
if (v < lmin) lmin = v;
if (v > lmax) lmax = v;
}

#pragma omp critical
{
if (lmin < gmin) gmin = lmin;
if (lmax > gmax) gmax = lmax;
}
}
return {gmin, gmax};
}


Этот вариант даёт больше контроля над логикой слияния, но время работы сильнее зависит от стоимости входа в критическую секцию, особенно при большом числе потоков.

# Методика измерений

Для каждого набора параметров (размер массива, число потоков, выбранный вариант алгоритма) генерировался массив псевдослучайных чисел с равномерным распределением значений. Для оценки устойчивых значений времени каждый эксперимент повторялся несколько раз, после чего вычислялось среднее.

Последовательный вариант запускался отдельно для каждого размера массива, его время работы использовалось как база для расчёта ускорения. Параллельные результаты, вместе с параметрами запуска, сохранялись в таблицу `bm_table.csv` со столбцами:

- `scenario` — тип реализации (`red` — редукция, `loc` — локальные экстремумы);
- `size` — размер массива;
- `workers` — число потоков;
- `time_sec` — среднее время выполнения;
- `speedup` — ускорение (отношение времени последовательного варианта к параллельному).

На основе этой таблицы Python-скрипт `draw_extrema_plots.py` строит набор графиков: для каждого размера — зависимости времени и ускорения от числа потоков, а также усреднённые по всем размерам кривые.

# Наблюдения по времени

Для самого маленького массива (\(n = 10^5\)) время выполнения быстро уменьшается при переходе от 1 к 2–4 потокам, но при дальнейшем увеличении числа потоков начинается рост времени. На графике `n_100000_time_sec.png` видно, что после некоторого порога накладные расходы на создание и синхронизацию потоков становятся сравнимы с полезными вычислениями, поэтому добавление потоков перестаёт быть выгодным.

Для размеров \(n = 10^6\) и \(5 \cdot 10^6\) (графики `n_1000000_time_sec.png` и `n_5000000_time_sec.png`) наблюдается более гладкое поведение: время монотонно снижается до 8–10 потоков и затем меняется незначительно. Кривые для `loc` и `red` практически совпадают, что говорит о близкой стоимости синхронизации в обоих вариантах при данных размерах задач.

При максимальном размере массива (\(n = 2 \cdot 10^7\)) обе реализации ведут себя почти одинаково (график `n_20000000_time_sec.png`). Время заметно падает при переходе к нескольким потокам, затем выходит на плато: дальнейшее увеличение числа потоков почти не влияет на длительность работы. В этом режиме узким местом становится пропускная способность подсистемы памяти, а не синхронизация.

# Наблюдения по ускорению

Графики ускорения (`n_100000_speedup.png`, `n_1000000_speedup.png`, `n_5000000_speedup.png`, `n_20000000_speedup.png`) демонстрируют, что:

- для \(n = 10^5\) ускорение остаётся близким к единице и чувствительно к числу потоков; локальные максимумы достигаются в районе 4–8 потоков, но при 16–20 потоках выигрыш практически исчезает или даже переходит в замедление;
- при \(n = 10^6\) и \(5 \cdot 10^6\) удаётся получить устойчивое ускорение порядка 4–5 раз при 8–10 потоках; дальнейшее наращивание числа потоков даёт лишь небольшое улучшение;
- для \(n = 2 \cdot 10^7\) максимальное ускорение достигает примерно 6–7 раз, после чего кривая также выходит на плато.

Усреднённые по всем размерам графики (`mean_time_sec.png` и `mean_speedup.png`) показывают аналогичную картину: среднее время резко падает от 1 до 4–8 потоков и затем меняется слабо, а среднее ускорение достигает пика в том же диапазоне и дальше практически не растёт.

# Сравнение подходов

На малых задачах реализация с редукцией в среднем показывает немного более предсказуемое поведение и лучшую масштабируемость, так как полностью избегает явных критических секций. В варианте с локальными экстремумами вклад секции `critical` в общее время оказывается заметным при большом числе потоков, что проявляется в более выраженном росте времени для малых задач.

На средних и крупных массивах разница между реализациями уменьшается: по графикам видно, что точки для `loc` и `red` почти совпадают по времени и ускорению при одинаковом числе потоков. В этих условиях на первый план выходит ограничение по памяти и общая организация доступа к данным, а детали синхронизации играют второстепенную роль.

# Выводы

1. Для малых размеров задач распараллеливание даёт ограниченный выигрыш: накладные расходы на создание и синхронизацию потоков могут перекрывать полезное ускорение.
2. При увеличении размера массива эффективность параллельной обработки возрастает: для крупнейшего размера достигается ускорение порядка 6–7 раз по сравнению с однопоточной реализацией.
3. Реализация, использующая редукции OpenMP, позволяет уменьшить влияние синхронизации и чуть лучше ведёт себя на малых задачах, однако при больших размерах массива обе параллельные версии демонстрируют очень близкую производительность.
4. Оптимальный диапазон числа потоков для рассматриваемой системы лежит примерно между 4 и 10: в этой области достигается наибольшее снижение времени и максимальное ускорение; дальнейшее увеличение числа потоков не даёт заметного выигрыша.
