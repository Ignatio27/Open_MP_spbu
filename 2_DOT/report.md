# Описание эксперимента

Задача — сравнить эффективность параллельного вычисления скалярного произведения двух вещественных векторов с помощью OpenMP. Исследуются две схемы распараллеливания: реализация с редукцией (`red`) и вариант с ручным накоплением частичных сумм (`acc` / `loc`, в зависимости от графика).

Измерения проводились для длин векторов \(10^5\), \(10^6\), \(5 \cdot 10^6\) и \(2 \cdot 10^7\) элементов. Для каждого размера запускались опыты при различном числе потоков (от 1 до 20), после чего вычислялось время выполнения и ускорение относительно однопоточного варианта. Все результаты сведены в таблицу `bm_table.csv` и визуализированы на графиках `n_*_time_sec.jpg`, `n_*_speedup.jpg` и усреднённых кривых `mean_time_sec.jpg`, `mean_speedup.jpg`.

# Суть решения

1. **Последовательная версия.**  
   Эталонный алгоритм — один проход по парам элементов двух векторов с накоплением суммы произведений. Эта реализация не использует OpenMP и даёт базовое время для расчёта ускорения.

2. **Вариант с редукцией (`red`).**  
   Основная работа выполняется в цикле `parallel for` с редукцией по переменной суммы. Каждый поток обрабатывает свою часть вектора и ведёт локальный аккумулятор, а рантайм OpenMP автоматически объединяет локальные значения в глобальный результат.

double sum = 0.0;
omp_set_num_threads(nthreads);

#pragma omp parallel for reduction(+ : sum) schedule(static)
for (int i = 0; i < static_cast<int>(x.size()); ++i) {
sum += x[i] * y[i];
}

3. **Вариант с ручным накоплением (`acc` / `loc`).**  
   В этом случае создаётся параллельная область, внутри которой каждый поток считает локальную сумму, а затем один раз заходит в критическую секцию и добавляет её к глобальному аккумулятору.

double global_sum = 0.0;
omp_set_num_threads(nthreads);

#pragma omp parallel
{
double local_sum = 0.0;
   #pragma omp for schedule(static)
   for (int i = 0; i < static_cast<int>(x.size()); ++i) {
       local_sum += x[i] * y[i];
   }

   #pragma omp critical
   {
       global_sum += local_sum;
   }
}


Обе параллельные версии вычисляют тот же результат, что и последовательная; при каждом запуске проверяется совпадение с эталоном с точностью до \(10^{-6}\).

# Анализ результатов

## Усреднённые графики

На усреднённом по всем размерам задач графике ускорения видно, что при переходе от 1 к 4 потокам достигается рост скорости примерно до 4–4.5×, а дальнейшее увеличение числа потоков до 20 даёт ускорение порядка 5–5.5×. При этом кривые для схем `acc/loc` и `red` почти совпадают: редукция даёт лишь небольшое преимущество на больших количествах потоков.

Усреднённое время выполнения падает особенно резко между 1 и 4 потоками и затем выходит на пологое плато: при 10–20 потоках среднее время уменьшается примерно до 0.0015–0.002 секунды, после чего дальнейшее увеличение числа потоков уже практически не чувствуется.

## Малые задачи (n = 100 000)

Для коротких векторов ускорение ведёт себя нестабильно: максимум достигается в районе 4 потоков и составляет около 2–2.3×, после чего при 10–20 потоках кривая идёт вниз.На графиках времени видно, что после 4 потоков время начинает расти — накладные расходы на создание потоков и синхронизацию становятся сопоставимыми с полезными вычислениями.

При таком размере задачи обе реализации по сути ограничены служебными затратами. `red` чуть устойчивее на больших количествах потоков, так как избегает частых входов в критическую секцию, но в целом для \(n = 10^5\) распараллеливание даёт лишь умеренный выигрыш.

## Средние задачи (n = 10^6 и 5·10^6)

В диапазоне \(10^6\)–\(5 \cdot 10^6\) наблюдается наиболее «красивое» масштабирование. Для \(n = 10^6\) ускорение растёт почти линейно до 8–10 потоков, достигая 4.5–5×, после чего плавно выходит на плато около 4–4.5× для 20 потоков. Для \(n = 5 \cdot 10^6\) максимальное ускорение ещё выше: около 6–6.5× при 12–20 потоках.

На графиках времени для этих размеров видно, что при увеличении числа потоков от 1 до 8 время снижается на порядок, затем уменьшается более медленно; кривые для `acc/loc` и `red` практически совпадают, что говорит о близкой стоимости синхронизации в обоих вариантах.
## Крупные задачи (n = 2·10^7)

Для самого большого вектора оба метода демонстрируют существенное ускорение: при 8–10 потоках достигается примерно 9–9.8×, а при 20 потоках — около 10–10.5×. Разница между `acc/loc` и `red` невелика, редукция иногда даёт немного большее ускорение, но характер кривых остаётся одинаковым.

Времена выполнения для \(n = 2 \cdot 10^7\) показывают резкое снижение от 0.025 секунды на одном потоке до 0.004–0.005 секунды при 10–20 потоках. После этого дальнейшее наращивание параллелизма практически не улучшает время: производительность ограничивается пропускной способностью памяти, а не затратами на синхронизацию.

# Итоги

1. **Малые векторы (\(n = 10^5\)).**  
Выигрыш от распараллеливания невелик: максимум порядка 2× при 4 потоках, затем рост числа потоков ухудшает результат. Для таких размеров доля накладных расходов OpenMP (создание потоков и синхронизация) сравнима с временем вычисления скалярного произведения.

2. **Средние размеры (\(10^6\)–\(5 \cdot 10^6\)).**  
Здесь параллельные версии наиболее эффективны: ускорения 4–5× для \(n = 10^6\) и до 6–6.5× для \(n = 5 \cdot 10^6\) при 8–12 потоках. Обе схемы (`acc/loc` и `red`) ведут себя похоже, редукция лишь немного устойчивее на больших количествах потоков.

3. **Крупные векторы (\(n = 2 \cdot 10^7\)).**  
Ускорение достигает 10× и выше, после чего почти не растёт с увеличением числа потоков, что указывает на ограничение по пропускной способности памяти. Различия между вариантами с редукцией и ручным накоплением здесь минимальны.

4. **Сравнение схем OpenMP.**  
Реализация с редукцией избавляет от явных критических секций и немного лучше масштабируется на больших числах потоков, однако при средних и крупных задачах её преимущество над вариантом с ручным накоплением невелико. Оба подхода дают сопоставимую производительность и корректно реализуют параллельное вычисление скалярного произведения.