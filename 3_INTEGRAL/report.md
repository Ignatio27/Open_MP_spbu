# Постановка задачи

Цель работы — исследовать, как ведёт себя параллельная программа для численного вычисления определённого интеграла при изменении числа потоков и размера задачи. Рассматривается интеграл вида  
\[
\int_a^b f(x)\,dx \approx h \sum_{i=0}^{N-1} f(x_i), \quad h = \frac{b-a}{N}, \quad x_i = a + i h
\]
и используется схема средних прямоугольников (mid–point rule), то есть фактически берётся значение функции в центре каждого отрезка разбиения.

В качестве подынтегральной функции выбрана \(f(x) = \sin x\), интегрирование проводится на отрезке \([0, \pi]\), для которого аналитическое значение интеграла равно 2. Это позволяет легко контролировать точность: каждое численное значение сравнивается с эталоном, а модуль ошибки сохраняется в таблицу результатов.

Исследуются четыре масштаба задачи: \(N = 10^5, 10^6, 5\cdot 10^6, 2\cdot 10^7\) узлов равномерной сетки. Для каждого размера запускается набор экспериментов при числе потоков \(p \in \{1,2,4,5,8\}\). По итогам каждого запуска в файл `bm_table.csv` записываются время выполнения, оценка интеграла, абсолютная ошибка и ускорение \(S = T_{\text{seq}} / T_{\text{par}}\) относительно предварительно измеренного однопоточного времени для данного N.

# Метод и программная реализация

## Численная схема

Базовый численный алгоритм представляет собой простой цикл по всем прямоугольникам. В последовательной версии на каждой итерации вычисляется координата центра прямоугольника и значение функции, после чего сумма умножается на шаг сетки:

double h = (b - a) / static_cast<double>(N);
double sum = 0.0;

for (long long i = 0; i < N; ++i) {
double x = a + (i + 0.5) * h;
sum += std::sin(x);
}
double integral = sum * h;


Такая структура цикла хорошо подходит для распараллеливания: отдельные итерации полностью независимы, нет обращений к общим структурам данных, кроме аккумулятора суммы. Это означает, что при переходе к многопоточной версии основным источником синхронизации становится только накопление частичных сумм.

## Параллельная версия с OpenMP

Для параллелизации используется директива `parallel for` с редукцией по переменной `sum`. В OpenMP механизм reduction решает сразу обе задачи: распределяет итерации цикла между потоками и аккуратно объединяет локальные суммы по завершении цикла. Фрагмент кода:

double h = (b - a) / static_cast<double>(N);
double sum = 0.0;

omp_set_num_threads(threads);

#pragma omp parallel for reduction(+:sum) schedule(static)
for (long long i = 0; i < N; ++i) {
double x = a + (i + 0.5) * h;
sum += std::sin(x);
}
double integral = sum * h;

Каждый поток получает собственную копию `sum`, с которой он работает без блокировок, а затем рантайм выполняет финальное сложение этих копий в одну переменную. Такая реализация минимизирует накладные расходы синхронизации и почти не усложняет исходный код по сравнению с последовательным вариантом.

## Организация бенчмарка и формат данных

Программа умеет работать в двух режимах: одиночный запуск с заданными параметрами и серия измерений для построения таблицы бенчмарков. Во втором случае в цикле перебираются все нужные значения N и числа потоков, для каждого случая сначала замеряется «чистое» последовательное время, а затем проводится несколько параллельных запусков с усреднением времени.

Строки CSV-файла имеют вид:
func,rule,N,threads,time_sec,estimate,abs_err,speedup_vs_seq
sin,mid,100000,4,0.000511,2.000000000082,0.000000,1.812133
...


Здесь `time_sec` — среднее время одного параллельного прогона для данной конфигурации, а `speedup_vs_seq` — отношение предварительно измеренного однопоточного времени к этому среднему значению.Такой формат облегчает построение графиков и вычисление усреднённых характеристик.

Для визуализации используется отдельный скрипт на Python, который группирует строки по (func, rule, N), строит зависимости времени и ускорения от числа потоков и дополнительно считает средние значения по всем N для фиксированного числа потоков. Итоговые изображения `*_time.png`, `*_speedup.png` и `mean_*` используются в отчёте для иллюстрации масштабируемости программы.

# Анализ усреднённых результатов

Усреднение по всем размерам задач позволяет посмотреть на общую тенденцию без привязки к конкретному N. На графике среднего ускорения для функции `sin` с правилом `mid` видно, что при переходе от одного потока к двум ускорение возрастает примерно до 1.6×, а на четырёх потоках достигает порядка 2×. Дальнейший рост числа потоков до пяти и восьми почти не меняет ситуацию — кривая выходит на плато.

Усреднённый график времени имеет похожую форму: первое резкое падение приходится на переход от одного потока к двум, затем заметное улучшение наблюдается при увеличении до четырёх потоков, после чего времена на 5 и 8 потоках отличаются уже незначительно.Это типичное поведение для задач, в которых объём вычислений хорошо делится на потоки, но подсистема памяти ограничивает масштабируемость после некоторого числа параллельных потоков.

С практической точки зрения усреднённые результаты показывают, что оптимальная зона работы приложения лежит в районе четырёх потоков: именно там достигается компромисс между накладными расходами и пропускной способностью памяти. Запуск на большем числе потоков лишь сильнее загружает процессор, но почти не улучшает время.

# Влияние размера задачи

## Малые размеры: \(N = 10^5\)

Для наименьшего размера сетки каждый эксперимент занимает доли миллисекунды. В таких условиях накладные расходы на создание и запуск потоков сравнимы с полезным временем вычислений. График ускорения показывает, что максимум достигается примерно на четырёх потоках и составляет около 1.8×, после чего при увеличении количества потоков до восьми заметного выигрыша уже нет.

График времени для этого N демонстрирует, что даже однопоточная версия работает очень быстро, а все вариации времени на разных конфигурациях укладываются в очень узкий интервал.Отсюда вывод: для столь малых задач распараллеливание не оправдано — сложность управления потоками инициализации среды OpenMP доминирует над полезной работой.

## Средние размеры: \(N = 10^6\) и \(N = 5\cdot 10^6\)

При увеличении числа узлов сетки до миллиона и пяти миллионов каждое измерение становится достаточно долгим, чтобы накладные расходы «размазались» по времени эксперимента. Для этих размеров ускорение растёт более ровно и достигает примерно 2× на четырёх потоках как для \(10^6\), так и для \(5\cdot 10^6\) элементов.

Графики времени показывают выраженный минимум также в районе четырёх потоков: дальше время либо практически не меняется, либо слегка увеличивается из‑за конкуренции потоков за общую шину памяти и кэш-подсистему.Такое поведение соответствует ожиданиям: объём работы уже достаточен для эффективной загрузки нескольких ядер, но пропускная способность памяти по прежнему ограничивает прирост производительности при дальнейшем увеличении числа потоков.

## Крупный размер: \(N = 2\cdot 10^7\)

Наибольший интерес представляет поведение программы на самом крупном размере сетки. Здесь каждый запуск уже занимает десятки миллисекунд, и влияние фонового шума заметно меньше. По данным таблицы `bm_table.csv` видно, что однопоточное время составляет порядка 0.16 секунды, тогда как на четырёх и пяти потоках оно снижается до 0.07–0.08 секунды, что даёт ускорение около 2.1–2.2×.

При дальнейшем увеличении числа потоков до восьми существенного выигрыша добиться не удаётся: времена остаются на том же уровне, а ускорение практически не растёт.Это указывает на то, что процессор уже упёрся в пропускную способность подсистемы памяти и кешей: дополнительным потокам просто не хватает «данных из памяти», чтобы выполнять полезные операции без ожидания.

# Обсуждение и выводы

Анализ всех графиков и таблиц позволяет выделить несколько ключевых моментов. Во‑первых, выбранная задача интегрирования по формуле средних прямоугольников хорошо подходит для демонстрации параллельных вычислений: тело цикла простое, операции над элементами независимы, а единственный источник синхронизации — суммирование результата — изящно решается редукцией OpenMP.

Во‑вторых, эксперименты показывают классическую картину масштабируемости:  
– для малых задач распараллеливание почти не даёт выигрыша;  
– при средних размерах достигается заметное ускорение до 2×;  
– при дальнейшем увеличении числа потоков рост производительности останавливается из‑за ограничений по памяти.

В‑третьих, использование только одного варианта параллелизации (через `reduction`) оказалось достаточным: редукция обеспечивает простой и надёжный способ распараллеливания без явных критических секций и ручного управления локальными буферами. При желании можно было бы сравнить этот подход с альтернативами (например, ручным разбиением диапазона интегрирования и явным накоплением частичных сумм в массиве), однако текущие результаты уже хорошо иллюстрируют основные эффекты.

Итоговый вывод: метод прямоугольников является практически идеально параллелимым по вычислительной части, но реальное ускорение на рассматриваемом процессоре ограничивается приблизительно двукратным значением. Причина не в алгоритме, а в аппаратных характеристиках системы — прежде всего в пропускной способности памяти и эффективности кэширования при последовательном обходе большого массива значений функции.
