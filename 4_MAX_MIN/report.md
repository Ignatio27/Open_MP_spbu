# Постановка

Рассматривается задача нахождения максимума среди построчных минимумов элементов матрицы:
\[
y = \max_{1 \le i \le N} \min_{1 \le j \le N} a_{ij}.
\]
Такая конструкция появляется, например, в простейших постановках матричных игр, где строка соответствует стратегии игрока, а выбираемый минимум — «наихудший исход» при оптимальных действиях противника.

В работе исследуется производительность параллельной реализации этого алгоритма на квадратных матрицах размером \(1000 \times 1000\), \(10000 \times 10000\), \(15000 \times 15000\) и \(20000 \times 20000\). Для каждого размера выполнены запуски на 1, 2, 4, 5 и 8 потоках, измерено время и вычислено ускорение относительно последовательной версии.

# Реализация алгоритма

## Последовательный вариант

Матрица хранится в виде одномерного массива построчно. Последовательный код последовательно обходит строки, для каждой строки ищет минимум, после чего обновляет глобальный максимум из найденных минимумов.

double maximin_sequential(const MatrixData& a, int rows, int cols) {
double best = -std::numeric_limits<double>::infinity();
for (int i = 0; i < rows; ++i) {
double row_min = std::numeric_limits<double>::infinity();
int base = i * cols;
for (int j = 0; j < cols; ++j) {
double v = a[base + j];
if (v < row_min) row_min = v;
}
if (row_min > best) best = row_min;
}
return best;
}

Эта реализация служит как для проверки корректности, так и для получения базового времени работы, используемого в формуле ускорения.

## Параллельная версия с редукцией

Задача естественным образом распадается по строкам: вычисление минимума в одной строке никак не влияет на остальные. В параллельной версии каждая строка обрабатывается отдельной итерацией цикла `parallel for`, а объединение результатов выполняется с помощью редукции по максимуму.

double maximin_parallel(const MatrixData& a,
int rows, int cols,
int threads) {
double global_best = -std::numeric_limits<double>::infinity();
omp_set_num_threads(threads);
#pragma omp parallel for reduction(max : global_best) schedule(static)
for (int i = 0; i < rows; ++i) {
    double row_min = std::numeric_limits<double>::infinity();
    int base = i * cols;
    for (int j = 0; j < cols; ++j) {
        double v = a[base + j];
        if (v < row_min) row_min = v;
    }
    if (row_min > global_best) {
        global_best = row_min;
    }
}
return global_best;
}

Редукция избавляет от явных критических секций: каждый поток работает со своей копией `global_best`, а после цикла рантайм OpenMP автоматически находит максимум этих локальных значений.

## Генерация данных и режим бенчмарка

Матрица заполняется псевдослучайными вещественными числами из равномерного распределения на отрезке \([-10^6, 10^6]\) с фиксированным зерном генератора, чтобы результаты были воспроизводимыми.Программа поддерживает два режима:

- одиночный запуск с параметрами `--rows`, `--cols`, `--threads`;
- серия измерений `--bench`, при которой:

  - для каждого размера матрицы сначала несколько раз (по умолчанию три) измеряется время последовательной функции;
  - затем для каждого числа потоков выполняется по три параллельных прогона с усреднением времени;
  - результаты сохраняются в CSV-формате
    `nrows,ncols,threads,time_sec,speedup_vs_seq`.

Формат таблицы совместим с питоновским скриптом визуализации.

# Построение графиков

Скрипт `plot_bm.py` читает `bm_table.csv`, проверяет наличие всех необходимых колонок и добавляет вспомогательное поле `n = nrows`, так как матрицы квадратные. Далее строятся:

- для каждого размера матрицы по два графика:
  - зависимость времени от числа потоков;
  - зависимость ускорения от числа потоков;
- усреднённые по всем размерам графики времени и ускорения.

Изображения сохраняются в каталог `charts` в файлах `n*_time.png`, `n*_speedup.png`, `mean_time.png`, `mean_speedup.png`, которые далее используются в отчёте.

# Анализ результатов

1. Усреднённые показатели
Усреднённый по всем размерам матриц график времени показывает практически монотонное уменьшение времени выполнения при увеличении числа потоков от 1 до 8: кривая идёт плавно вниз без выраженного плато. Среднее ускорение при этом растёт почти линейно и на 8 потоках достигает значения более 7 относительно последовательной версии.​

Такое поведение говорит о том, что в рассматриваемом диапазоне потоков накладные расходы распараллеливания не успевают «съесть» выигрыш, а подсистема памяти справляется с дополнительной нагрузкой без заметного насыщения. Алгоритм остаётся эффективно параллелизуемым для всех протестированных конфигураций.

2. Матрица 1000×1000
Для небольшого размера 1000×1000 времена работы лежат в диапазоне от примерно 0.0018 c на одном потоке до порядка 2.7⋅10**−4
  c на восьми потоках. При этом график ускорения практически линейный: на 8 потоках достигается ускорение около 7.2 раз по сравнению с одним потоком.​

Несмотря на небольшой объём данных, накладные расходы OpenMP здесь оказываются относительно малы по сравнению с полезной работой. Даже для такой матрицы масштабирование остаётся близким к идеальному, что показывает хорошее соотношение вычислений и коммуникаций.

3. Матрица 10000×10000
Для матрицы 10000×10000 время выполнения снижается с примерно 0.125 c до ~0.017 c при росте числа потоков от 1 до 8. Ускорение также растёт почти по прямой и достигает величины порядка 7.5 на восьми потоках.​

Здесь видно, что по мере увеличения размера задачи параллельная версия использует ресурсы процессора ещё эффективнее: каждая нить получает больше работы, и относительное влияние накладных расходов дальше уменьшается. В то же время признаков насыщения пропускной способности памяти на графиках пока не наблюдается.

4. Матрица 15000×15000
Для размера 15000×15000 картина похожа: время уменьшается от ~0.28 c до ~0.04 c при переходе с одного на восемь потоков. График ускорения остаётся почти идеальной прямой, достигая значений чуть более 7 на максимальном числе потоков.​

Это подтверждает, что алгоритм «максимум из построчных минимумов» хорошо масштабируется на крупных матрицах: вычислительная нагрузка на каждую строку достаточно велика, а доступы к памяти имеют регулярный, предсказуемый характер, что помогает кэшам.

5. Матрица 20000×20000
Наибольший размер 20000×20000 демонстрирует ещё более впечатляющее снижение времени: с ~0.5 c на одном потоке до приблизительно 0.067 c на восьми потоках. Соответствующий график ускорения даёт почти идеальную линейную зависимость с ростом до значений порядка 7.5.​

Отсутствие перегиба или плато даже на самой крупной матрице говорит о том, что для тестируемого числа потоков bottleneckом остаются вычисления, а не пропускная способность памяти. Потоки успевают эффективно перерабатывать данные, не простаивая в ожидании доступа к RAM.

Итоговые выводы
Для всех исследованных размеров матриц (от 1000×1000 до 20000×20000) наблюдается почти линейный рост ускорения при увеличении числа потоков с 1 до 8 и монотонное уменьшение времени работы, без выраженного плато или деградации производительности.​

Даже для самой маленькой матрицы параллельная версия демонстрирует значимый выигрыш (ускорение более 7 раз на 8 потоках), что означает, что накладные расходы OpenMP и синхронизаций невелики относительно объёма вычислений в каждой задаче строки.​

Для средних и больших матриц эффективность параллельного алгоритма остаётся высокой: время работы уменьшается почти пропорционально числу потоков, а получаемое ускорение близко к теоретическому максимуму, что указывает на хорошую балансировку нагрузки и регулярный доступ к памяти.​

В пределах рассмотренного диапазона потоков ограничений со стороны пропускной способности памяти заметно не проявляется: ни один из графиков не показывает насыщения или ухудшения времени при переходе на большее число потоков. Это позволяет сделать вывод, что дальнейшее увеличение числа потоков (на более многопроцессорных системах) имеет шанс дать дополнительный прирост, пока объём данных и архитектура памяти это позволяют.​

